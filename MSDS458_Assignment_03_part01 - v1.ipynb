{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alawrence30/Deep-Learning/blob/main/MSDS458_Assignment_03_part01%20-%20v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hu2sg27u4w7"
      },
      "source": [
        "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/NorthwesternHeader.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHpI6-3Bu4w_"
      },
      "source": [
        "## MSDS458 Research Assignment 3 - Part 01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FomIU6qDu4xA"
      },
      "source": [
        "## Analyze AG_NEWS_SUBSET Data <br>\n",
        "\n",
        "AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity.<br> \n",
        "\n",
        "For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html<br> \n",
        "\n",
        "\n",
        "The AG's news topic classification dataset is constructed by choosing 4 largest classes (**World**, **Sports**, **Business**, and **Sci/Tech**) from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.<br>\n",
        "\n",
        "Homepage: https://arxiv.org/abs/1509.01626<br>\n",
        "\n",
        "Source code: tfds.text.AGNewsSubset\n",
        "\n",
        "Versions:\n",
        "\n",
        "1.0.0 (default): No release notes.\n",
        "Download size: 11.24 MiB\n",
        "\n",
        "Dataset size: 35.79 MiB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AD9D4Klu4xB"
      },
      "source": [
        "## References\n",
        "1. Deep Learning with Python, Francois Chollet (https://learning.oreilly.com/library/view/deep-learning-with/9781617296864/)\n",
        " * Chapter 10: Deep learning for time series\n",
        " * Chapter 11: Deep learning for text\n",
        "2. Deep Learning A Visual Approach, Andrew Glassner (https://learning.oreilly.com/library/view/deep-learning/9781098129019/)\n",
        " * Chapter 19: Recurrent Neural Networks\n",
        " * Chapter 20: Attention and Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMI67-qiu4xB"
      },
      "source": [
        "# Deep learning for text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U7VH-F2u4xC"
      },
      "source": [
        "## Natural-language processing: The bird's eye view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKGFiCLQu4xC"
      },
      "source": [
        "## Preparing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYDZmgm0u4xD"
      },
      "source": [
        "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/11-01.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND-XyAu5u4xD"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QSrVxNoOu4xE"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDVrqTMsu4xF"
      },
      "source": [
        "## Verify TensorFlow version and Keras version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N9OxjtLu4xF",
        "outputId": "11ab69e5-1f15-4430-9d9d-70fbbaca7915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This notebook requires TensorFlow 2.0 or above\n",
            "TensorFlow version:  2.9.2\n"
          ]
        }
      ],
      "source": [
        "print(\"This notebook requires TensorFlow 2.0 or above\")\n",
        "print(\"TensorFlow version: \", tf.__version__)\n",
        "assert version.parse(tf.__version__).release[0] >=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BweLTYUlu4xG",
        "outputId": "53db6389-cb2d-4807-ba49-f7d022fe7af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras version:  2.9.0\n"
          ]
        }
      ],
      "source": [
        "print(\"Keras version: \", keras.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7UV14cFu4xG"
      },
      "source": [
        "## Mount Google Drive to Colab environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0vOWAjQu4xH",
        "outputId": "357a1c99-3273-4e49-e1b8-9065d2159ea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwu-QgiVu4xH"
      },
      "source": [
        "## Load AG_NEWS_SUBSET News Articles Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ5KvOIVu4xH",
        "outputId": "65dedab4-0b9d-45a8-a555-ee80cdd47795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-29 21:54:04.216012: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "W1029 21:54:04.216403 140474941478784 download_and_prepare.py:43] ***`tfds build` should be used instead of `download_and_prepare`.***\n",
            "INFO[build.py]: Loading dataset ag_news_subset from imports: tensorflow_datasets.text.ag_news_subset\n",
            "2022-10-29 21:54:04.348648: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"NOT_FOUND: Error executing an HTTP request: HTTP response code 404\".\n",
            "INFO[dataset_info.py]: Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: ag_news_subset/1.0.0\n",
            "INFO[dataset_info.py]: Load dataset info from /tmp/tmpr03q029ftfds\n",
            "INFO[dataset_info.py]: Field info.splits from disk and from code do not match. Keeping the one from code.\n",
            "INFO[dataset_info.py]: Field info.supervised_keys from disk and from code do not match. Keeping the one from code.\n",
            "INFO[dataset_info.py]: Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
            "INFO[build.py]: download_and_prepare for dataset ag_news_subset/1.0.0...\n",
            "INFO[dataset_builder.py]: Generating dataset ag_news_subset (/root/tensorflow_datasets/ag_news_subset/1.0.0)\n",
            "\u001b[1mDownloading and preparing dataset 11.24 MiB (download: 11.24 MiB, generated: 35.79 MiB, total: 47.03 MiB) to /root/tensorflow_datasets/ag_news_subset/1.0.0...\u001b[0m\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "                                       \n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[AINFO[download_manager.py]: Downloading https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbUDNpeUdjb0wxRms into /root/tensorflow_datasets/downloads/ucexport_download_id_0Bz8a_Dbh9QhbUDNpeUdjb0wxj4g1umFAV8OV-uDwxSJR0LdxO_k1jxMuFWwAfNX9jos.tmp.c7e493062569486ea030f9ee25f8e965...\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:00<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:12<?, ? url/s]\n",
            "Dl Size...:   0% 0/11 [00:12<?, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:12, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:   0% 0/1 [00:12<?, ? url/s]\n",
            "Dl Size...:   9% 1/11 [00:12<02:02, 12.28s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:12<?, ? url/s]\n",
            "Dl Size...:  18% 2/11 [00:12<01:50, 12.28s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:12<?, ? url/s]\n",
            "Dl Size...:  27% 3/11 [00:12<01:38, 12.28s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:12<?, ? url/s]\n",
            "Dl Size...:  36% 4/11 [00:12<01:25, 12.28s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:12<?, ? url/s]\n",
            "Dl Size...:  45% 5/11 [00:12<01:13, 12.28s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:12<?, ? url/s]\n",
            "Dl Size...:  55% 6/11 [00:12<01:01, 12.28s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:12<?, ? url/s]\n",
            "Dl Size...:  64% 7/11 [00:12<00:49, 12.28s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:12<?, ? url/s]\n",
            "Dl Size...:  73% 8/11 [00:12<00:36, 12.28s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:12<?, ? url/s]\n",
            "Dl Size...:  82% 9/11 [00:12<00:24, 12.28s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:12<?, ? url/s]\n",
            "Dl Size...:  91% 10/11 [00:12<00:12, 12.28s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:12<?, ? url/s]\n",
            "Dl Size...: 100% 11/11 [00:12<00:00, 12.28s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:12<00:00, 12.38s/ url]\n",
            "Dl Size...: 100% 11/11 [00:12<00:00, 12.28s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:12<00:00, 12.38s/ url]\n",
            "Dl Size...: 100% 11/11 [00:12<00:00, 12.28s/ MiB]\u001b[A\n",
            "\n",
            "Extraction completed...:   0% 0/1 [00:12<?, ? file/s]\u001b[A\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:12<00:00, 12.38s/ url]\n",
            "Dl Size...: 100% 11/11 [00:12<00:00, 12.28s/ MiB]\u001b[A\n",
            "\n",
            "Extraction completed...: 100% 1/1 [00:12<00:00, 12.64s/ file]\n",
            "Dl Size...: 100% 11/11 [00:12<00:00,  1.15s/ MiB]\n",
            "Dl Completed...: 100% 1/1 [00:12<00:00, 12.65s/ url]\n",
            "Generating splits...:   0% 0/2 [00:00<?, ? splits/s]\n",
            "Generating train examples...:   0% 0/120000 [00:00<?, ? examples/s]\u001b[A\n",
            "Generating train examples...:   1% 1015/120000 [00:00<00:11, 10142.37 examples/s]\u001b[A\n",
            "Generating train examples...:   2% 2030/120000 [00:00<00:12, 9526.56 examples/s] \u001b[A\n",
            "Generating train examples...:   2% 2986/120000 [00:00<00:12, 9529.79 examples/s]\u001b[A\n",
            "Generating train examples...:   3% 3941/120000 [00:00<00:12, 9100.49 examples/s]\u001b[A\n",
            "Generating train examples...:   4% 4969/120000 [00:00<00:12, 9504.80 examples/s]\u001b[A\n",
            "Generating train examples...:   5% 5946/120000 [00:00<00:11, 9592.00 examples/s]\u001b[A\n",
            "Generating train examples...:   6% 6954/120000 [00:00<00:11, 9747.85 examples/s]\u001b[A\n",
            "Generating train examples...:   7% 8001/120000 [00:00<00:11, 9972.28 examples/s]\u001b[A\n",
            "Generating train examples...:   8% 9000/120000 [00:00<00:11, 9868.21 examples/s]\u001b[A\n",
            "Generating train examples...:   8% 9989/120000 [00:01<00:11, 9662.54 examples/s]\u001b[A\n",
            "Generating train examples...:   9% 10957/120000 [00:01<00:11, 9608.85 examples/s]\u001b[A\n",
            "Generating train examples...:  10% 11919/120000 [00:01<00:11, 9388.93 examples/s]\u001b[A\n",
            "Generating train examples...:  11% 12927/120000 [00:01<00:11, 9591.24 examples/s]\u001b[A\n",
            "Generating train examples...:  12% 13920/120000 [00:01<00:10, 9689.15 examples/s]\u001b[A\n",
            "Generating train examples...:  12% 14920/120000 [00:01<00:10, 9780.13 examples/s]\u001b[A\n",
            "Generating train examples...:  13% 15932/120000 [00:01<00:10, 9879.59 examples/s]\u001b[A\n",
            "Generating train examples...:  14% 16925/120000 [00:01<00:10, 9893.60 examples/s]\u001b[A\n",
            "Generating train examples...:  15% 17934/120000 [00:01<00:10, 9950.15 examples/s]\u001b[A\n",
            "Generating train examples...:  16% 18941/120000 [00:01<00:10, 9985.86 examples/s]\u001b[A\n",
            "Generating train examples...:  17% 19940/120000 [00:02<00:10, 9865.45 examples/s]\u001b[A\n",
            "Generating train examples...:  17% 20928/120000 [00:02<00:10, 9837.44 examples/s]\u001b[A\n",
            "Generating train examples...:  18% 21913/120000 [00:02<00:10, 9535.98 examples/s]\u001b[A\n",
            "Generating train examples...:  19% 22869/120000 [00:02<00:10, 9271.77 examples/s]\u001b[A\n",
            "Generating train examples...:  20% 23836/120000 [00:02<00:10, 9384.18 examples/s]\u001b[A\n",
            "Generating train examples...:  21% 24834/120000 [00:02<00:09, 9555.87 examples/s]\u001b[A\n",
            "Generating train examples...:  22% 25843/120000 [00:02<00:09, 9710.62 examples/s]\u001b[A\n",
            "Generating train examples...:  22% 26853/120000 [00:02<00:09, 9823.89 examples/s]\u001b[A\n",
            "Generating train examples...:  23% 27882/120000 [00:02<00:09, 9961.76 examples/s]\u001b[A\n",
            "Generating train examples...:  24% 28880/120000 [00:02<00:09, 9831.75 examples/s]\u001b[A\n",
            "Generating train examples...:  25% 29865/120000 [00:03<00:09, 9787.31 examples/s]\u001b[A\n",
            "Generating train examples...:  26% 30845/120000 [00:03<00:09, 9710.24 examples/s]\u001b[A\n",
            "Generating train examples...:  27% 31817/120000 [00:03<00:09, 9580.85 examples/s]\u001b[A\n",
            "Generating train examples...:  27% 32856/120000 [00:03<00:08, 9818.04 examples/s]\u001b[A\n",
            "Generating train examples...:  28% 33839/120000 [00:03<00:08, 9685.97 examples/s]\u001b[A\n",
            "Generating train examples...:  29% 34809/120000 [00:03<00:09, 9369.35 examples/s]\u001b[A\n",
            "Generating train examples...:  30% 35798/120000 [00:03<00:08, 9517.17 examples/s]\u001b[A\n",
            "Generating train examples...:  31% 36752/120000 [00:03<00:08, 9367.00 examples/s]\u001b[A\n",
            "Generating train examples...:  31% 37716/120000 [00:03<00:08, 9443.86 examples/s]\u001b[A\n",
            "Generating train examples...:  32% 38725/120000 [00:04<00:08, 9633.03 examples/s]\u001b[A\n",
            "Generating train examples...:  33% 39690/120000 [00:04<00:08, 9514.24 examples/s]\u001b[A\n",
            "Generating train examples...:  34% 40689/120000 [00:04<00:08, 9651.89 examples/s]\u001b[A\n",
            "Generating train examples...:  35% 41656/120000 [00:04<00:08, 9610.50 examples/s]\u001b[A\n",
            "Generating train examples...:  36% 42618/120000 [00:04<00:08, 9427.94 examples/s]\u001b[A\n",
            "Generating train examples...:  36% 43562/120000 [00:04<00:08, 8885.74 examples/s]\u001b[A\n",
            "Generating train examples...:  37% 44515/120000 [00:04<00:08, 9067.11 examples/s]\u001b[A\n",
            "Generating train examples...:  38% 45453/120000 [00:04<00:08, 9155.90 examples/s]\u001b[A\n",
            "Generating train examples...:  39% 46392/120000 [00:04<00:07, 9222.28 examples/s]\u001b[A\n",
            "Generating train examples...:  40% 47429/120000 [00:04<00:07, 9557.47 examples/s]\u001b[A\n",
            "Generating train examples...:  40% 48388/120000 [00:05<00:07, 9534.24 examples/s]\u001b[A\n",
            "Generating train examples...:  41% 49344/120000 [00:05<00:07, 9302.87 examples/s]\u001b[A\n",
            "Generating train examples...:  42% 50292/120000 [00:05<00:07, 9353.71 examples/s]\u001b[A\n",
            "Generating train examples...:  43% 51230/120000 [00:05<00:07, 9050.14 examples/s]\u001b[A\n",
            "Generating train examples...:  43% 52176/120000 [00:05<00:07, 9167.13 examples/s]\u001b[A\n",
            "Generating train examples...:  44% 53226/120000 [00:05<00:06, 9554.36 examples/s]\u001b[A\n",
            "Generating train examples...:  45% 54185/120000 [00:05<00:06, 9488.93 examples/s]\u001b[A\n",
            "Generating train examples...:  46% 55136/120000 [00:05<00:06, 9444.23 examples/s]\u001b[A\n",
            "Generating train examples...:  47% 56153/120000 [00:05<00:06, 9657.92 examples/s]\u001b[A\n",
            "Generating train examples...:  48% 57121/120000 [00:05<00:06, 9160.73 examples/s]\u001b[A\n",
            "Generating train examples...:  48% 58043/120000 [00:06<00:07, 8808.20 examples/s]\u001b[A\n",
            "Generating train examples...:  49% 58930/120000 [00:06<00:07, 8448.63 examples/s]\u001b[A\n",
            "Generating train examples...:  50% 59781/120000 [00:06<00:07, 8152.17 examples/s]\u001b[A\n",
            "Generating train examples...:  51% 60744/120000 [00:06<00:06, 8560.18 examples/s]\u001b[A\n",
            "Generating train examples...:  51% 61705/120000 [00:06<00:06, 8855.31 examples/s]\u001b[A\n",
            "Generating train examples...:  52% 62713/120000 [00:06<00:06, 9206.75 examples/s]\u001b[A\n",
            "Generating train examples...:  53% 63770/120000 [00:06<00:05, 9604.00 examples/s]\u001b[A\n",
            "Generating train examples...:  54% 64782/120000 [00:06<00:05, 9751.38 examples/s]\u001b[A\n",
            "Generating train examples...:  55% 65766/120000 [00:06<00:05, 9775.62 examples/s]\u001b[A\n",
            "Generating train examples...:  56% 66747/120000 [00:07<00:05, 9696.65 examples/s]\u001b[A\n",
            "Generating train examples...:  56% 67719/120000 [00:07<00:05, 9469.51 examples/s]\u001b[A\n",
            "Generating train examples...:  57% 68709/120000 [00:07<00:05, 9593.96 examples/s]\u001b[A\n",
            "Generating train examples...:  58% 69671/120000 [00:07<00:05, 9514.21 examples/s]\u001b[A\n",
            "Generating train examples...:  59% 70624/120000 [00:07<00:05, 9237.47 examples/s]\u001b[A\n",
            "Generating train examples...:  60% 71594/120000 [00:07<00:05, 9371.32 examples/s]\u001b[A\n",
            "Generating train examples...:  61% 72613/120000 [00:07<00:04, 9608.86 examples/s]\u001b[A\n",
            "Generating train examples...:  61% 73584/120000 [00:07<00:04, 9638.51 examples/s]\u001b[A\n",
            "Generating train examples...:  62% 74564/120000 [00:07<00:04, 9684.15 examples/s]\u001b[A\n",
            "Generating train examples...:  63% 75567/120000 [00:07<00:04, 9786.37 examples/s]\u001b[A\n",
            "Generating train examples...:  64% 76547/120000 [00:08<00:04, 9749.33 examples/s]\u001b[A\n",
            "Generating train examples...:  65% 77547/120000 [00:08<00:04, 9823.47 examples/s]\u001b[A\n",
            "Generating train examples...:  65% 78565/120000 [00:08<00:04, 9927.93 examples/s]\u001b[A\n",
            "Generating train examples...:  66% 79559/120000 [00:08<00:04, 9681.45 examples/s]\u001b[A\n",
            "Generating train examples...:  67% 80565/120000 [00:08<00:04, 9791.75 examples/s]\u001b[A\n",
            "Generating train examples...:  68% 81581/120000 [00:08<00:03, 9899.30 examples/s]\u001b[A\n",
            "Generating train examples...:  69% 82612/120000 [00:08<00:03, 10020.46 examples/s]\u001b[A\n",
            "Generating train examples...:  70% 83616/120000 [00:08<00:03, 10025.78 examples/s]\u001b[A\n",
            "Generating train examples...:  71% 84660/120000 [00:08<00:03, 10147.57 examples/s]\u001b[A\n",
            "Generating train examples...:  71% 85676/120000 [00:08<00:03, 10104.29 examples/s]\u001b[A\n",
            "Generating train examples...:  72% 86687/120000 [00:09<00:03, 10039.86 examples/s]\u001b[A\n",
            "Generating train examples...:  73% 87724/120000 [00:09<00:03, 10135.22 examples/s]\u001b[A\n",
            "Generating train examples...:  74% 88738/120000 [00:09<00:03, 10101.38 examples/s]\u001b[A\n",
            "Generating train examples...:  75% 89749/120000 [00:09<00:03, 9788.76 examples/s] \u001b[A\n",
            "Generating train examples...:  76% 90736/120000 [00:09<00:02, 9810.25 examples/s]\u001b[A\n",
            "Generating train examples...:  76% 91719/120000 [00:09<00:02, 9793.77 examples/s]\u001b[A\n",
            "Generating train examples...:  77% 92700/120000 [00:09<00:02, 9633.72 examples/s]\u001b[A\n",
            "Generating train examples...:  78% 93665/120000 [00:09<00:02, 9305.16 examples/s]\u001b[A\n",
            "Generating train examples...:  79% 94626/120000 [00:09<00:02, 9392.28 examples/s]\u001b[A\n",
            "Generating train examples...:  80% 95607/120000 [00:10<00:02, 9513.20 examples/s]\u001b[A\n",
            "Generating train examples...:  81% 96631/120000 [00:10<00:02, 9725.59 examples/s]\u001b[A\n",
            "Generating train examples...:  81% 97606/120000 [00:10<00:02, 9503.70 examples/s]\u001b[A\n",
            "Generating train examples...:  82% 98559/120000 [00:10<00:02, 9330.35 examples/s]\u001b[A\n",
            "Generating train examples...:  83% 99497/120000 [00:10<00:02, 9344.03 examples/s]\u001b[A\n",
            "Generating train examples...:  84% 100474/120000 [00:10<00:02, 9468.40 examples/s]\u001b[A\n",
            "Generating train examples...:  85% 101450/120000 [00:10<00:01, 9553.06 examples/s]\u001b[A\n",
            "Generating train examples...:  85% 102407/120000 [00:10<00:01, 9308.43 examples/s]\u001b[A\n",
            "Generating train examples...:  86% 103391/120000 [00:10<00:01, 9461.53 examples/s]\u001b[A\n",
            "Generating train examples...:  87% 104359/120000 [00:10<00:01, 9525.06 examples/s]\u001b[A\n",
            "Generating train examples...:  88% 105313/120000 [00:11<00:01, 9423.68 examples/s]\u001b[A\n",
            "Generating train examples...:  89% 106257/120000 [00:11<00:01, 9288.76 examples/s]\u001b[A\n",
            "Generating train examples...:  89% 107207/120000 [00:11<00:01, 9349.00 examples/s]\u001b[A\n",
            "Generating train examples...:  90% 108194/120000 [00:11<00:01, 9498.76 examples/s]\u001b[A\n",
            "Generating train examples...:  91% 109145/120000 [00:11<00:01, 9238.17 examples/s]\u001b[A\n",
            "Generating train examples...:  92% 110099/120000 [00:11<00:01, 9324.70 examples/s]\u001b[A\n",
            "Generating train examples...:  93% 111104/120000 [00:11<00:00, 9537.46 examples/s]\u001b[A\n",
            "Generating train examples...:  93% 112121/120000 [00:11<00:00, 9721.83 examples/s]\u001b[A\n",
            "Generating train examples...:  94% 113104/120000 [00:11<00:00, 9744.45 examples/s]\u001b[A\n",
            "Generating train examples...:  95% 114082/120000 [00:11<00:00, 9752.39 examples/s]\u001b[A\n",
            "Generating train examples...:  96% 115087/120000 [00:12<00:00, 9839.22 examples/s]\u001b[A\n",
            "Generating train examples...:  97% 116072/120000 [00:12<00:00, 9668.23 examples/s]\u001b[A\n",
            "Generating train examples...:  98% 117040/120000 [00:12<00:00, 9503.77 examples/s]\u001b[A\n",
            "Generating train examples...:  98% 117993/120000 [00:12<00:00, 9510.36 examples/s]\u001b[A\n",
            "Generating train examples...:  99% 118945/120000 [00:12<00:00, 8380.49 examples/s]\u001b[A\n",
            "Generating train examples...: 100% 119859/120000 [00:12<00:00, 8583.89 examples/s]\u001b[A\n",
            "                                                                                  \u001b[A\n",
            "Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteEFGMWZ/ag_news_subset-train.tfrecord*...:   0% 0/120000 [00:00<?, ? examples/s]\u001b[A\n",
            "Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteEFGMWZ/ag_news_subset-train.tfrecord*...:   0% 1/120000 [00:00<5:59:45,  5.56 examples/s]\u001b[A\n",
            "Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteEFGMWZ/ag_news_subset-train.tfrecord*...:  10% 12011/120000 [00:00<00:02, 52178.68 examples/s]\u001b[A\n",
            "Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteEFGMWZ/ag_news_subset-train.tfrecord*...:  19% 23300/120000 [00:00<00:01, 75408.52 examples/s]\u001b[A\n",
            "Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteEFGMWZ/ag_news_subset-train.tfrecord*...:  49% 58204/120000 [00:00<00:00, 172146.12 examples/s]\u001b[A\n",
            "Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteEFGMWZ/ag_news_subset-train.tfrecord*...:  85% 102517/120000 [00:00<00:00, 263074.96 examples/s]\u001b[A\n",
            "INFO[writer.py]: Done writing /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteEFGMWZ/ag_news_subset-train.tfrecord*. Number of examples: 120000 (shards: [120000])\n",
            "Generating splits...:  50% 1/2 [00:13<00:13, 13.29s/ splits]\n",
            "Generating test examples...:   0% 0/7600 [00:00<?, ? examples/s]\u001b[A\n",
            "Generating test examples...:  12% 922/7600 [00:00<00:00, 9202.72 examples/s]\u001b[A\n",
            "Generating test examples...:  24% 1843/7600 [00:00<00:00, 9063.17 examples/s]\u001b[A\n",
            "Generating test examples...:  36% 2765/7600 [00:00<00:00, 8121.78 examples/s]\u001b[A\n",
            "Generating test examples...:  47% 3588/7600 [00:00<00:00, 5925.62 examples/s]\u001b[A\n",
            "Generating test examples...:  60% 4541/7600 [00:00<00:00, 6914.62 examples/s]\u001b[A\n",
            "Generating test examples...:  73% 5512/7600 [00:00<00:00, 7701.36 examples/s]\u001b[A\n",
            "Generating test examples...:  86% 6507/7600 [00:00<00:00, 8345.78 examples/s]\u001b[A\n",
            "Generating test examples...:  98% 7479/7600 [00:00<00:00, 8743.65 examples/s]\u001b[A\n",
            "                                                                             \u001b[A\n",
            "Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteEFGMWZ/ag_news_subset-test.tfrecord*...:   0% 0/7600 [00:00<?, ? examples/s]\u001b[A\n",
            "INFO[writer.py]: Done writing /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteEFGMWZ/ag_news_subset-test.tfrecord*. Number of examples: 7600 (shards: [7600])\n",
            "\u001b[1mDataset ag_news_subset downloaded and prepared to /root/tensorflow_datasets/ag_news_subset/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "INFO[build.py]: Dataset generation complete...\n",
            "\n",
            "tfds.core.DatasetInfo(\n",
            "    name='ag_news_subset',\n",
            "    full_name='ag_news_subset/1.0.0',\n",
            "    description=\"\"\"\n",
            "    AG is a collection of more than 1 million news articles.\n",
            "    News articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity.\n",
            "    ComeToMyHead is an academic news search engine which has been running since July, 2004.\n",
            "    The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc),\n",
            "    information retrieval (ranking, search, etc), xml, data compression, data streaming,\n",
            "    and any other non-commercial activity.\n",
            "    For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\n",
            "    \n",
            "    The AG's news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above.\n",
            "    It is used as a text classification benchmark in the following paper:\n",
            "    Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n",
            "    \n",
            "    The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus.\n",
            "    Each class contains 30,000 training samples and 1,900 testing samples.\n",
            "    The total number of training samples is 120,000 and testing 7,600.\n",
            "    \"\"\",\n",
            "    homepage='https://arxiv.org/abs/1509.01626',\n",
            "    data_path='/root/tensorflow_datasets/ag_news_subset/1.0.0',\n",
            "    file_format=tfrecord,\n",
            "    download_size=11.24 MiB,\n",
            "    dataset_size=35.79 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'description': Text(shape=(), dtype=tf.string),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=4),\n",
            "        'title': Text(shape=(), dtype=tf.string),\n",
            "    }),\n",
            "    supervised_keys=('description', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=7600, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=120000, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"@misc{zhang2015characterlevel,\n",
            "        title={Character-level Convolutional Networks for Text Classification},\n",
            "        author={Xiang Zhang and Junbo Zhao and Yann LeCun},\n",
            "        year={2015},\n",
            "        eprint={1509.01626},\n",
            "        archivePrefix={arXiv},\n",
            "        primaryClass={cs.LG}\n",
            "    }\"\"\",\n",
            ")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# register  ag_news_subset so that tfds.load doesn't generate a checksum (mismatch) error\n",
        "!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=ag_news_subset\n",
        "\n",
        "dataset, info = tfds.load('ag_news_subset', with_info=True,  split=['train[:114000]','train[114000:]', 'test'],\n",
        "                          batch_size = 32, as_supervised=True)\n",
        "train_ds, val_ds, test_ds = dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WqZfHoLu4xH"
      },
      "source": [
        "## Display The Number of Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h4cmD3Hu4xI",
        "outputId": "ffb5c047-80df-476f-8078-88cb82dbb681"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3563, 188, 238)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(train_ds), len(val_ds), len(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbzyOD6Fu4xI"
      },
      "source": [
        "## Displaying The Shapes and Dtypes of the First Batch"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "8powdIdqu4xI"
      },
      "source": [
        "'description': Text(shape=(), dtype=tf.string),\n",
        "'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=4),\n",
        "'title': Text(shape=(), dtype=tf.string),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_1m0aEiu4xI",
        "outputId": "a5c66cc9-0bef-4800-9431-de0c1a8e8117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "\n",
            "targets.shape: (32,)\n",
            "\n",
            "targets.dtype: <dtype: 'int64'>\n",
            "\n",
            "inputs[0]: tf.Tensor(b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.', shape=(), dtype=string)\n",
            "\n",
            "targets[0]: tf.Tensor(3, shape=(), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print()\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print()\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print()\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print()\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print()\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3wESs2mu4xI"
      },
      "source": [
        "## Processing words as a set: The bag-of-words approach\n",
        "\n",
        "The simplest way to encode a piece of text for processing by a machine learning model is to discard order and treat it as a set (a “bag”) of tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MiBRvRZu4xJ"
      },
      "source": [
        "## Single words (unigrams) with binary encoding\n",
        "\n",
        "The main advantage of this encoding is that you can represent an entire text as a single vector, where each entry is a presence indicator for a given word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6qYEr3Ou4xJ"
      },
      "source": [
        "## Preprocessing Datasets TextVectorization Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRW1DsJhu4xJ"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    <b>tf.keras.layers.TextVectorization</b><br>\n",
        "    https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "ch9JQxQ6u4xJ"
      },
      "source": [
        "Max Token = 1000 Word Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IsxZJoC4u4xK"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=1000,\n",
        "    output_mode=\"multi_hot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "94ZFb3bVu4xK"
      },
      "outputs": [],
      "source": [
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO3F3VD2u4xK",
        "outputId": "a8083e4d-1863-4b51-9c98-f4ee2085b88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get first batch of 32 news articles.\n",
            "\n",
            "Here is the first news article:\n",
            "\n",
            "b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.'.\n"
          ]
        }
      ],
      "source": [
        "for text in text_only_train_ds:\n",
        "    print(f\"Get first batch of {text.shape[0]} news articles.\\n\")\n",
        "    print(f\"Here is the first news article:\\n\\n{text[0]}.\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10QkZMcEu4xK"
      },
      "source": [
        "## Adapt Method - Standardize Text"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "G-TSWmviu4xK"
      },
      "source": [
        "Create the layer, and pass the dataset's text to the layer's `.adapt` method:\n",
        "The processing of each sample contains the following steps:\n",
        "\n",
        "    standardize each sample (usually lowercasing + punctuation stripping)\n",
        "    split each sample into substrings (usually words)\n",
        "    recombine substrings into tokens (usually ngrams)\n",
        "    index tokens (associate a unique int value with each token)\n",
        "    transform each sample using this index, either into a vector of ints or a dense float vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6jKg2o6Yu4xL"
      },
      "outputs": [],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gLoS8oDRu4xL"
      },
      "outputs": [],
      "source": [
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j6cOFceu4xL"
      },
      "source": [
        "## Inspecting Output Binary Unigram Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YL1LNyfu4xL",
        "outputId": "af7edf5a-4cf6-49f1-8f6d-ee62daf68650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 1000)\n",
            "\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "\n",
            "targets.shape: (32,)\n",
            "\n",
            "targets.dtype: <dtype: 'int64'>\n",
            "\n",
            "targets[0]: tf.Tensor(3, shape=(), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print()\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print()\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print()\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print()\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OcYJXwou4xL"
      },
      "source": [
        "## Model Function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YWrw1CCTu4xL"
      },
      "outputs": [],
      "source": [
        "def get_model(max_tokens=1000, hidden_dim=16):\n",
        "    inputs = tf.keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss='SparseCategoricalCrossentropy',\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geBUp675u4xM"
      },
      "source": [
        "## Build Binary Unigram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_nYePh6u4xM",
        "outputId": "9a752741-1681-4d8a-d54d-3f0844413bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1000)]            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                16016     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 68        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,084\n",
            "Trainable params: 16,084\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_Unigram = get_model()\n",
        "model_Unigram.summary()\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",save_best_only=True)\n",
        "    ,tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2ftXQbEu4xM",
        "outputId": "f68b2de8-5ee0-4652-d50d-851d7ae74b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "3563/3563 [==============================] - 8s 2ms/step - loss: 0.6795 - accuracy: 0.7571 - val_loss: 0.4687 - val_accuracy: 0.8483\n",
            "Epoch 2/200\n",
            "3563/3563 [==============================] - 5s 1ms/step - loss: 0.5980 - accuracy: 0.7992 - val_loss: 0.4841 - val_accuracy: 0.8407\n",
            "Epoch 3/200\n",
            "3563/3563 [==============================] - 5s 1ms/step - loss: 0.6050 - accuracy: 0.7967 - val_loss: 0.4871 - val_accuracy: 0.8395\n",
            "Epoch 4/200\n",
            "3563/3563 [==============================] - 4s 1ms/step - loss: 0.6075 - accuracy: 0.7948 - val_loss: 0.4865 - val_accuracy: 0.8387\n",
            "Epoch 5/200\n",
            "3563/3563 [==============================] - 4s 1ms/step - loss: 0.6060 - accuracy: 0.7947 - val_loss: 0.4808 - val_accuracy: 0.8377\n",
            "Epoch 6/200\n",
            "3563/3563 [==============================] - 6s 2ms/step - loss: 0.6009 - accuracy: 0.7963 - val_loss: 0.4825 - val_accuracy: 0.8417\n",
            "238/238 [==============================] - 1s 2ms/step - loss: 0.4956 - accuracy: 0.8375\n",
            "Test acc: 0.837\n"
          ]
        }
      ],
      "source": [
        "model_Unigram.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=200,\n",
        "          callbacks=callbacks)\n",
        "model_Unigram = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model_Unigram.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbonRnHbu4xM"
      },
      "source": [
        "We call `cache()` on the datasets to cache them in memory: this way, we will only do the preprocessing once, during the first epoch, and we’ll reuse the preprocessed texts for the following epochs. This can only be done if the data is small enough to fit in memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXqALfpju4xN"
      },
      "source": [
        "## Bigrams With Binary Encoding\n",
        "\n",
        "Of course, discarding word order is very reductive, because even atomic concepts can be expressed via multiple words: the term “United States” conveys a concept that is quite distinct from the meaning of the words “states” and “united” taken separately. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sqkt0Gfu4xN"
      },
      "source": [
        "With bigrams, the sentence “`the cat sat on the mat.`” becomes\n",
        "\n",
        "`{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n",
        " \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYePeGuqu4xN"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    <b>tf.keras.layers.TextVectorization</b><br>\n",
        "    https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44wKtsl2u4xN"
      },
      "source": [
        "## Configuring the `TextVectorization` layer to return Bigrams\n",
        "\n",
        "The TextVectorization layer can be configured to return arbitrary N-grams: bigrams, trigrams, etc. Just pass an `ngrams=N` argument as in the following listing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sAMv5pAdu4xN"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=1000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mubdnyau4xN"
      },
      "source": [
        "## Build Binary Bigram Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHOdCXFFu4xO",
        "outputId": "bd41857b-da52-4613-a982-b793c35ab8fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 1000)]            0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                16016     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4)                 68        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,084\n",
            "Trainable params: 16,084\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model_Bigram = get_model()\n",
        "model_Bigram.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35rLhp9xu4xO",
        "outputId": "0aedfec1-9dd1-4442-e479-2d1a82fc5799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "3563/3563 [==============================] - 7s 2ms/step - loss: 0.7098 - accuracy: 0.7399 - val_loss: 0.4796 - val_accuracy: 0.8425\n",
            "Epoch 2/200\n",
            "3563/3563 [==============================] - 5s 1ms/step - loss: 0.6227 - accuracy: 0.7892 - val_loss: 0.4870 - val_accuracy: 0.8395\n",
            "Epoch 3/200\n",
            "3563/3563 [==============================] - 5s 1ms/step - loss: 0.6302 - accuracy: 0.7885 - val_loss: 0.4896 - val_accuracy: 0.8363\n",
            "Epoch 4/200\n",
            "3563/3563 [==============================] - 5s 1ms/step - loss: 0.6303 - accuracy: 0.7854 - val_loss: 0.4884 - val_accuracy: 0.8357\n",
            "Epoch 5/200\n",
            "3563/3563 [==============================] - 5s 1ms/step - loss: 0.6279 - accuracy: 0.7854 - val_loss: 0.4882 - val_accuracy: 0.8370\n",
            "Epoch 6/200\n",
            "3563/3563 [==============================] - 5s 1ms/step - loss: 0.6254 - accuracy: 0.7861 - val_loss: 0.4874 - val_accuracy: 0.8353\n",
            "238/238 [==============================] - 0s 1ms/step - loss: 0.5023 - accuracy: 0.8325\n",
            "Test acc: 0.832\n"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "     tf.keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",save_best_only=True)\n",
        "    ,tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "]\n",
        "\n",
        "model_Bigram.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=200,\n",
        "          callbacks=callbacks)\n",
        "model_Bigram = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model_Bigram.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa1pk1XHu4xO"
      },
      "source": [
        "## Bigrams with TF-IDF Encoding\n",
        "\n",
        "You can also add a bit more information to this representation by counting how many times each word or N-gram occurs, that is to say, by taking the histogram of the words over the text:\n",
        "\n",
        "```{\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1,\n",
        " \"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syJyyCtsu4xO"
      },
      "source": [
        "## Understanding TF-IDF normalization\n",
        "The more a given term appears in a document, the more important that term is for understanding what the document is about. At the same time, the frequency at which the term appears across all documents in your dataset matters too: terms that appear in almost every document (like “the” or “a”) aren’t particularly informative,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGVyIS0ku4xP"
      },
      "source": [
        "`TF-IDF` is a metric that fuses these two ideas. It weights a given term by taking “term frequency,” how many times the term appears in the current document, and dividing it by a measure of “document frequency,” which estimates how often the term comes up across the dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maUZjyuQu4xP"
      },
      "source": [
        "```python\n",
        "def tfidf(term, document, dataset):\n",
        "    term_freq = document.count(term)\n",
        "    doc_freq = math.log(sum(doc.count(term) for doc in dataset) + 1)\n",
        "    return term_freq / doc_freq\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwbLdFDPu4xP"
      },
      "source": [
        "## Configure `TextVectorization` Layer To Return Token Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0SKpB0A0u4xP"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=1000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppHHy_S8u4xP"
      },
      "source": [
        "## Configuring `TextVectorization` To Return TF-IDF-weighted Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VKq9BaQAu4xP"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=1000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYHKrbAIu4xP"
      },
      "source": [
        "## Build TF-IDF Bigram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwe6JZ9-u4xQ",
        "outputId": "b68a060c-4dc3-4748-875a-8b8134b15e44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 1000)]            0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                16016     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4)                 68        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,084\n",
            "Trainable params: 16,084\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model_tfidf = get_model()\n",
        "model_tfidf.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VETX0Vv7u4xQ",
        "outputId": "b7c76531-4678-4bd3-ad58-937072713998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "3563/3563 [==============================] - 7s 2ms/step - loss: 0.9767 - accuracy: 0.5629 - val_loss: 0.5775 - val_accuracy: 0.8177\n",
            "Epoch 2/200\n",
            "3563/3563 [==============================] - 5s 1ms/step - loss: 0.8714 - accuracy: 0.6257 - val_loss: 0.5960 - val_accuracy: 0.7920\n",
            "Epoch 3/200\n",
            "3563/3563 [==============================] - 4s 1ms/step - loss: 0.8235 - accuracy: 0.6620 - val_loss: 0.5641 - val_accuracy: 0.8127\n",
            "Epoch 4/200\n",
            "3563/3563 [==============================] - 4s 1ms/step - loss: 0.7991 - accuracy: 0.6765 - val_loss: 0.5508 - val_accuracy: 0.8208\n",
            "Epoch 5/200\n",
            "3563/3563 [==============================] - 5s 1ms/step - loss: 0.7930 - accuracy: 0.6777 - val_loss: 0.5551 - val_accuracy: 0.8182\n",
            "Epoch 6/200\n",
            "3563/3563 [==============================] - 4s 1ms/step - loss: 0.7894 - accuracy: 0.6818 - val_loss: 0.5479 - val_accuracy: 0.8190\n",
            "Epoch 7/200\n",
            "3563/3563 [==============================] - 5s 1ms/step - loss: 0.7829 - accuracy: 0.6859 - val_loss: 0.5497 - val_accuracy: 0.8205\n",
            "Epoch 8/200\n",
            "3563/3563 [==============================] - 4s 1ms/step - loss: 0.7859 - accuracy: 0.6854 - val_loss: 0.5364 - val_accuracy: 0.8283\n",
            "Epoch 9/200\n",
            "3563/3563 [==============================] - 5s 1ms/step - loss: 0.7831 - accuracy: 0.6862 - val_loss: 0.5494 - val_accuracy: 0.8223\n",
            "Epoch 10/200\n",
            "3563/3563 [==============================] - 5s 1ms/step - loss: 0.7832 - accuracy: 0.6880 - val_loss: 0.5498 - val_accuracy: 0.8207\n",
            "Epoch 11/200\n",
            "3563/3563 [==============================] - 5s 1ms/step - loss: 0.7823 - accuracy: 0.6887 - val_loss: 0.5404 - val_accuracy: 0.8267\n",
            "Epoch 12/200\n",
            "3563/3563 [==============================] - 4s 1ms/step - loss: 0.7853 - accuracy: 0.6873 - val_loss: 0.5433 - val_accuracy: 0.8242\n",
            "Epoch 13/200\n",
            "3563/3563 [==============================] - 5s 1ms/step - loss: 0.7845 - accuracy: 0.6864 - val_loss: 0.5613 - val_accuracy: 0.8207\n",
            "238/238 [==============================] - 0s 1ms/step - loss: 0.5494 - accuracy: 0.8129\n",
            "Test acc: 0.813\n"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",save_best_only=True)\n",
        "   ,tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "]\n",
        "\n",
        "model_tfidf.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=200,\n",
        "          callbacks=callbacks)\n",
        "model_tfidf = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model_tfidf.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XQXk6KQCu4xQ"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model_tfidf(processed_inputs)\n",
        "inference_model = tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbScMr6Ju4xQ",
        "outputId": "4abf3422-576a-47aa-95c7-f3ee9d2b7971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_3 (TextV  (None, 1000)             1         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " model_2 (Functional)        (None, 4)                 16084     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,085\n",
            "Trainable params: 16,084\n",
            "Non-trainable params: 1\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inference_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8ZABay3u4xQ",
        "outputId": "f4a12ebd-eb29-4726-9683-b608260230e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24.58 percent positive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.2458025 , 0.23040332, 0.19021425, 0.33357987], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "raw_text_data = tf.convert_to_tensor(\n",
        "    [[\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{predictions.numpy()[0][0] * 100:.2f} percent positive\")\n",
        "predictions.numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxQqKgVbu4xR",
        "outputId": "95bfa00a-58a1-4363-9b25-0c527a57e9cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40.94 percent positive\n"
          ]
        }
      ],
      "source": [
        "raw_text_data = tf.convert_to_tensor([['''\n",
        "ATLANTA -- Atlanta Braves shortstop Rafael Furcal has had his first court appearance\n",
        "after being arrested on charges of driving under the influence.'\n",
        "''']])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{predictions.numpy()[0][0] * 100:.2f} percent positive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWTYTs_2u4xR",
        "outputId": "967b4dea-ca3c-488c-8639-60b94796cc48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.40942684, 0.32446882, 0.10614656, 0.15995784]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "predictions.numpy()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}